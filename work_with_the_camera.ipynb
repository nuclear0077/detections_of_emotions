{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#установим библиотеки для работы с моделью \n",
    "# !pip install git+https://github.com/rcmalli/keras-vggface.git\n",
    "# !pip install keras_vggface\n",
    "# !pip install keras_applications\n",
    "# !pip install  image\n",
    "# !pip install q keras==2.2.4\n",
    "# conda install -c conda-forge keras-applications \n",
    "# pip install keras-vggface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем библиотеки\n",
    "import tensorflow as tf\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface import utils\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image as image_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузим наш словарь полученный при подготовки данных перед обучением модели \n",
    "#для дальнейшей расшифроки эмоций в текстовом виде\n",
    "with open('./model/class_dict.pickle', 'rb') as f:\n",
    "    class_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'anger',\n",
       " 1: 'contempt',\n",
       " 2: 'disgust',\n",
       " 3: 'fear',\n",
       " 4: 'happy',\n",
       " 5: 'neutral',\n",
       " 6: 'sad',\n",
       " 7: 'surprise',\n",
       " 8: 'uncertain'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#посмотрим на классы\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_emotions:\n",
    "    \"\"\"\n",
    "    Класс который при инициализации загружают обученную модель \n",
    "    Функция prepare_image подготавливает картинку для подачи в нейросеть\n",
    "    Функция predict_emotions делает прогноз и переводит эмоцию в текст \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.load_model('./model/checkpoint_best.hdf5')\n",
    "    \n",
    "    def prepare_image(self,image):\n",
    "        #меняем размер картинки\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "        #разворачиваем массив\n",
    "        img = np.expand_dims(resized_image, axis=0) \n",
    "        #меняем формат\n",
    "        img = img.astype('float64')\n",
    "        img = utils.preprocess_input(img, version=2) # мы используем resnet50 -- поэтому version2\n",
    "        return img\n",
    "    \n",
    "    def predict_emotion(self,image):\n",
    "        predicted = self.model.predict(image)\n",
    "        predicted_emotion = class_dict.get(np.argmax(predicted))\n",
    "        return predicted_emotion\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создадим класс модели\n",
    "model = model_emotions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#подключимся к камере\n",
    "cam = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting_out_the_face(img,faces):\n",
    "    \"\"\"\n",
    "    Функция для вырезания лица из картинки\n",
    "    Перевода из формата BGR в RGB\n",
    "    Далее вызываем подготовки картинки для модели и прогноза\n",
    "    Функция возвращает класс эмоции\n",
    "    \"\"\"\n",
    "    #переводим картинку в RGP в формат\n",
    "    rgb_frame = img[:, :, ::-1]\n",
    "    one_face = faces[0]\n",
    "    x, y, w, h = one_face\n",
    "    face_boundingbox_bgr = img[y:y + h, x:x + w]\n",
    "    face_boundingbox_rgb = cv2.cvtColor(face_boundingbox_bgr, cv2.COLOR_BGR2RGB)\n",
    "    #подготавливаем картинку\n",
    "    image_to_model = model.prepare_image(face_boundingbox_rgb)\n",
    "    #получаем прогноз\n",
    "    cls_predicted = model.predict_emotion(image_to_model)\n",
    "    return cls_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "#здесь мы в бесконечном цыкле пока пользователь не нажмет (esc) считываем кадры с камеры\n",
    "while True:\n",
    "    #получим изображение с камеры\n",
    "    ret,img = cam.read()\n",
    "    #загрузим детектор лиц\n",
    "    faces = face_detector.detectMultiScale(img,scaleFactor=1.5,minNeighbors=5,minSize=(20,20))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        #добавим boundingbox\n",
    "        cv2.rectangle(img,(x, y), (x+w, y+h),(0,255,0),3)\n",
    "        #вызываем функцию для получения эмоции\n",
    "        emo = cutting_out_the_face(img,faces)\n",
    "        #вставим текст полученной эмоции\n",
    "        cv2.putText(img, emo, (y + 150, x - 140), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0),2)\n",
    "    #визуализируем картинку\n",
    "    cv2.imshow('From Camera', img)\n",
    "    #если нажали кнопку esc, то цикл прерывается\n",
    "    k = cv2.waitKey(30) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "#закрываем все окна\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam = cv2.VideoCapture(0)\n",
    "# # cam.set(cv2.CAP_PROP_FRAME_WIDTH, 240)  # ширина кадра -- 640 пикселей\n",
    "# # cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)  # высота кадра -- 480 пикселей\n",
    "# def emotions(img,faces):\n",
    "#     rgb_frame = img[:, :, ::-1]\n",
    "#     one_face = faces[0]\n",
    "#     x, y, w, h = one_face\n",
    "#     face_boundingbox_bgr = img[y:y + h, x:x + w]\n",
    "#     face_boundingbox_rgb = cv2.cvtColor(face_boundingbox_bgr, cv2.COLOR_BGR2RGB)\n",
    "#     im = image_n.fromarray(face_boundingbox_rgb)\n",
    "#     im.save(\"temp.jpeg\")\n",
    "#     #print(type(face_boundingbox_rgb))\n",
    "# #     img_1 = image.load_img('temp.jpeg', target_size=(224, 224))\n",
    "# #     x_1 = image.img_to_array(img_1) #переводим картинку в массив\n",
    "# #     x_1 = np.expand_dims(x_1, axis=0) #разворачиваем массив\n",
    "# #     x_1 = utils.preprocess_input(x_1, version=2) # мы используем resnet50 -- поэтому version2\n",
    "# #     pred = model_load.predict(x_1)\n",
    "# #     cls = class_dict.get(np.argmax(pred))\n",
    "#     return cls\n",
    "# #     plt.imshow(x_1)\n",
    "# #     plt.show()\n",
    "\n",
    "# face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# while True:\n",
    "#     ret,img = cam.read()\n",
    "\n",
    "#     faces = face_detector.detectMultiScale(img,scaleFactor=1.5,minNeighbors=5,minSize=(20,20))\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         cv2.rectangle(img,(x, y), (x+w, y+h),(0,255,0),3)\n",
    "#         #cv2.rectangle(img, (y, x), (y + h, x + w), (0, 255, 0), 3)\n",
    "#         emo = emotions(img,faces)\n",
    "#         cv2.putText(img, emo, (y + 250, x - 140), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0),2)\n",
    "\n",
    "#     cv2.imshow('From Camera', img)\n",
    "\n",
    "#     k = cv2.waitKey(30) & 0xFF\n",
    "#     if k == 27:\n",
    "#         break\n",
    "\n",
    "# cam.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
